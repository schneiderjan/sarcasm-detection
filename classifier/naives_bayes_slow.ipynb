{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import nltk\n",
    "import sklearn as sk\n",
    "from time import gmtime, strftime\n",
    "from pprint import pprint\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Properties\n",
    "py_file = \"NaiveBayesClassifier.py\"\n",
    "date_file = \"data/tweets_90-10.csv\"\n",
    "start_time = time.time()\n",
    "testd_size = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999\n"
     ]
    }
   ],
   "source": [
    "# Read Data Set\n",
    "df = pd.read_csv(date_file, delimiter='|', encoding=\"utf-8\", quotechar='|', header=None, names=['ID', 'Tweet','Status'])\n",
    "#df = df.dropna()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweets Train Set :  3249\n",
      "Tweets Test  Set :  1750\n"
     ]
    }
   ],
   "source": [
    "# Create Train and Test Set\n",
    "train_df, test_df = train_test_split(df, test_size = testd_size)\n",
    "print (\"\\nTweets Train Set : \",len(train_df))\n",
    "print (\"Tweets Test  Set : \",len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Start Training : 3249  Tweets (96.81184601783752 Seconds)\n"
     ]
    }
   ],
   "source": [
    "print (\"\\n\\nStart Training :\", len(train_df), \" Tweets (%s Seconds)\" % (time.time() - start_time))\n",
    "tweets_train = []\n",
    "for row in train_df.itertuples():\n",
    "    words_filtered = [e.lower() for e in row[2].split() if len(e) >= 3] \n",
    "    tweets_train.append((words_filtered, row[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Set - Tweets\n",
    "tweets_test = []\n",
    "for row in test_df.itertuples():\n",
    "    words_filtered = [e.lower() for e in row[2].split() if len(e) >= 3] \n",
    "    tweets_test.append((words_filtered, row[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Unique Word from Word List\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the word lists of Tweets\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "      all_words.extend(words)\n",
    "    return all_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_features = get_word_features(get_words_in_tweets(tweets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Extractor\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "      features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build Training and Test Set\n",
    "training_set = nltk.classify.util.apply_features(extract_features, tweets_train)\n",
    "test_set = nltk.classify.util.apply_features(extract_features, tweets_test)\n",
    "\n",
    "# Create Classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK Train Method\n",
    "def train(labeled_featuresets, estimator=nltk.probability.ELEProbDist):\n",
    "    \n",
    "    # Create the P(label) distribution\n",
    "    label_probdist = estimator(label_freqdist)\n",
    "    \n",
    "    # Create the P(fval|label, fname) distribution\n",
    "    feature_probdist = {}\n",
    "    return NaiveBayesClassifier(label_probdist, feature_probdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Start Testing 1 : 1750  Tweets (266.39972591400146 Seconds)\n",
      "Wrong Predicted Tweets: \n",
      "Prediction : 165 wrong from 1750\n",
      "Accuracy   : 0.9057142857142857\n"
     ]
    }
   ],
   "source": [
    "# Display\n",
    "print (\"\\n\\nStart Testing 1 :\", len(test_df), \" Tweets (%s Seconds)\" % (time.time() - start_time))\n",
    "print (\"Wrong Predicted Tweets: \")\n",
    "count = 0\n",
    "for row in test_df.itertuples():\n",
    "    a = classifier.classify(extract_features(row[2].split()))\n",
    "    if row[3] != a:\n",
    "        #print(row[2])\n",
    "        #print(\"Label:      \", row[3])\n",
    "        #print(\"Prediction: \", a)\n",
    "        #print(\"\\n\")\n",
    "        count += 1\n",
    "accuracy_test1 = (len(test_df)-count)/len(test_df)\n",
    "print (\"Prediction : %d wrong from %d\" %(count, len(test_df)))\n",
    "print (\"Accuracy   :\", (accuracy_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'classifier/classifier2015-11-24_11:41:10.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b2a3d72958b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Save Classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'classifier/classifier%s.pkl'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y-%m-%d_%H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\jan\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, cache_size)\u001b[0m\n\u001b[0;32m    368\u001b[0m                                cache_size=cache_size)\n\u001b[0;32m    369\u001b[0m         \u001b[0mpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0mpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'pickler'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jan\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mzfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m                 \u001b[0mwrite_zfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'classifier/classifier2015-11-24_11:41:10.pkl'"
     ]
    }
   ],
   "source": [
    "# Save Classifier\n",
    "filename = 'classifier/classifier%s.pkl' % (str(time.strftime(\"%Y-%m-%d_%H:%M:%S\")))\n",
    "_ = joblib.dump(classifier, filename, compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print Output\n",
    "print (\"\\n\\nStart Testing 2 :\", len(test_df), \" Tweets (%s Seconds)\" % (time.time() - start_time))\n",
    "accuracy_test2 = nltk.classify.util.accuracy(classifier, test_set)\n",
    "print (\"Accuracy:\", accuracy_test2)\n",
    "print(classifier.most_informative_features(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check Accuracy Tests\n",
    "if accuracy_test1>accuracy_test2:\n",
    "\tprint \"\\nTest 1 is more accurate\"\n",
    "elif accuracy_test1<accuracy_test2:\n",
    "\tprint \"\\nTest 2 is more accurate\"\n",
    "else:\n",
    "\tprint \"\\nResults are the same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# End\n",
    "print(\"\\nTotal Time : %s Seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
